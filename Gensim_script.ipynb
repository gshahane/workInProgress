{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from parse import *\n",
    "from docx import Document\n",
    "from functools import reduce\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "def modify_working_dir(dir_name):\n",
    "    # Modify the path based on where you keep your docx files\n",
    "    os.chdir(dir_name)\n",
    "\n",
    "def filenames():\n",
    "    # Extracts filenames based on the working directory\n",
    "    #Print working directory for your reference\n",
    "    print(os.getcwd())\n",
    "    files = os.listdir(str(os.getcwd()))\n",
    "    docx = []\n",
    "    for file in files:\n",
    "        if file.endswith(\".docx\"):\n",
    "            docx.append(file)\n",
    "    return docx\n",
    "\n",
    "def word_doc(file):\n",
    "    # Combines paragraphs into one continuous string of text\n",
    "\n",
    "    #print(file)\n",
    "    f = Document(file)\n",
    "    textline = []\n",
    "    for para in f.paragraphs:\n",
    "        textline.append(para.text)\n",
    "    text = reduce(lambda x, y: x + ' ' + y, textline)\n",
    "    return text\n",
    "\n",
    "def parse_text(text, filename):\n",
    "    #Parses the individual document based on the search parameters in global list: list_headings\n",
    "\n",
    "    profile = {}\n",
    "    global list_headings\n",
    "    second = \": {}:\"\n",
    "    for _ in list_headings:\n",
    "        try:\n",
    "            profile[_] = (list(search(_.lower()+second, text.lower()))[0]).rsplit(\"  \",1)[0]\n",
    "        except:\n",
    "            profile[_] = \"None\"\n",
    "            profile[\"filename\"] = filename\n",
    "            pass\n",
    "    return profile\n",
    "\n",
    "def get_parsed():\n",
    "    #Parses all the documents and returns a pandas dataframe\n",
    "\n",
    "    final_dict = []\n",
    "    for filename in filenames():\n",
    "        text = word_doc(str(filename))\n",
    "        p = parse_text(text, filename)\n",
    "        final_dict.append(p)\n",
    "    #print(final_dict)\n",
    "    final_df = pd.DataFrame(final_dict)\n",
    "    df = final_df[list_headings]\n",
    "\n",
    "    return [df,\"dataframe\"]\n",
    "\n",
    "def create_corpus():\n",
    "    corpus = []\n",
    "    for _ in filenames():\n",
    "        text = word_doc(str(_))\n",
    "        corpus.append(text)\n",
    "    return [corpus, \"corpus\"]\n",
    "\n",
    "def modeling():\n",
    "    #Read Corpus\n",
    "    global topics_for_lda\n",
    "\n",
    "    #Read stopwords\n",
    "    with open(\"stopwords_custom.txt\", 'r') as f:\n",
    "        stopwords = f.read().split()\n",
    "        #print(stopwords)\n",
    "\n",
    "    modify_working_dir(result_dir)\n",
    "    corpus = pickle.load(open(\"corpus_phrack.p\", 'rb'))\n",
    "\n",
    "    #Count Term Frequency\n",
    "    tf_vectorizer = CountVectorizer(stop_words=stopwords, min_df= 5, max_df= 10,  ngram_range=(1,2))\n",
    "    tf = tf_vectorizer.fit_transform(corpus)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    #print(len(tf_feature_names))\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "    print (tfidf_matrix.shape)\n",
    "    \n",
    "    #Calculate Cosine Similarity\n",
    "    cs = cosine_similarity(tfidf_matrix[0:], tfidf_matrix)\n",
    "    cs = pd.DataFrame(cs)\n",
    "    \n",
    "    #LDA\n",
    "    lda = LatentDirichletAllocation(n_topics=topics_for_lda, max_iter = 500, learning_method='online', random_state=0 )\n",
    "    lda.fit(tf)\n",
    "    doc_word = lda.transform(tf)\n",
    "    #print(lda.transform(tf))\n",
    "\n",
    "    doc_topic_dist_unnormalized = np.matrix(doc_word)\n",
    "    \n",
    "    # normalize the distribution\n",
    "    doc_topic_dist = doc_topic_dist_unnormalized / doc_topic_dist_unnormalized.sum(axis=1)\n",
    "\n",
    "    doc_topic_dist = pd.DataFrame(doc_topic_dist*100)\n",
    "    return [lda, tf_feature_names, doc_topic_dist, cs]\n",
    "\n",
    "def print_top_words(model):\n",
    "    lda = model[0]\n",
    "    feature_names = model[1]\n",
    "    topic_dist = model[2]\n",
    "    cs = model[3]\n",
    "    n_top_words = 20\n",
    "\n",
    "    list_topic_words = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        topic_words = {}\n",
    "\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        words_for_topic = (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        print(words_for_topic)\n",
    "        topic_words[topic_idx] = words_for_topic\n",
    "        list_topic_words.append(topic_words)\n",
    "\n",
    "    #Read file for filenames and handle names\n",
    "    handles_df = pd.read_csv('handles.csv', sep=',')\n",
    "\n",
    "    # get topic distribution across the documents\n",
    "    list_topic_dist_docs = []\n",
    "\n",
    "    for _ in range(len(topic_dist.index)):\n",
    "        dict_topic_dist = {}\n",
    "        topic_for_doc= list(topic_dist.iloc[_,:])\n",
    "        try:\n",
    "            filename = handles_df.iloc[_,1].strip()\n",
    "            handle = handles_df.iloc[_,2].strip()\n",
    "        except:\n",
    "            handle = _\n",
    "\n",
    "        # Add lines for additional topics, if more than 3,  to be included in the final topic\n",
    "        dict_topic_dist['filename'] = filename\n",
    "        dict_topic_dist['handle'] = handle\n",
    "        for i in range(topics_for_lda):\n",
    "            topic_number = i+1\n",
    "            dict_topic_dist['topic_' + str(topic_number)] = topic_for_doc[i]\n",
    "        dict_topic_dist['top_topic'] = int(topic_for_doc.index(max(topic_for_doc))) + 1\n",
    "\n",
    "        #Append dictionary to the final list for dataframe creation\n",
    "        list_topic_dist_docs.append(dict_topic_dist)\n",
    "\n",
    "    #Sort and generate Topic Distribution File\n",
    "    df = pd.DataFrame(list_topic_dist_docs).sort(columns=\"top_topic\", ascending=True)\n",
    "    modify_working_dir(result_dir)\n",
    "    df.to_csv(\"topic distribution for prophiles.csv\")\n",
    "    #print(df.values)\n",
    "\n",
    "    #generate topics and their words distribution\n",
    "    df = pd.DataFrame(list_topic_words)\n",
    "    df.to_csv(\"topic-word distribution.csv\")\n",
    "    #df.reset_index()\n",
    "    \n",
    "    #generate cosine similarity csv\n",
    "    cs.to_csv(\"cosine similarity.csv\")\n",
    "        \n",
    "        \n",
    "def create_file(list):\n",
    "    # Writes the dataframe to a csv in the working directory\n",
    "    modify_working_dir(result_dir)\n",
    "    data = list[0]\n",
    "    type = list[1]\n",
    "    if type == \"dataframe\":\n",
    "        data.to_csv('prophiles.csv')\n",
    "        data1 = data[['filename', 'Handle']]\n",
    "        data1.to_csv(\"handles.csv\")\n",
    "    elif type == \"corpus\":\n",
    "        pickle.dump(data, open(\"corpus_phrack.p\", 'wb'))\n",
    "    else:\n",
    "        pass\n",
    "    modify_working_dir(data_dir)\n",
    "    return (str(type) + \" created\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #Add/Remove the headings to be parsed in the order for generating the last parsed dataframe\n",
    "    list_headings = [\"filename\", \"Handle\", \"Past handles\", \"Handle origin\", \"Call him\", \"Call her\", \"Age of your body\",\n",
    "         \"Date of Birth\", \"Produced in\", \"Height\", \"weight\", \"Height & Weight\", \"Eye color\", \"Hair Color\", \"Computers\",\n",
    "         \"Admin of\", \"Sites Frequented\", \"URLs\", \"Women\", \"Cars\", \"Foods\", \"Height & Weight\", \"Projects\", \"Alcohol\",\n",
    "         \"Books & Authors\", \"Authors\", \"Music\", \"Drugs\", \"I like\", \"I dislike\"]\n",
    "\n",
    "    #Variables\n",
    "    topics_for_lda = 10\n",
    "\n",
    "    create_csv = True\n",
    "    create_doc_corpus = True\n",
    "    create_topics = True\n",
    "\n",
    "    #working directories\n",
    "    os.chdir(\"C:\\\\Users\\\\gaura\\\\Documents\\\\MIM - UMCP\\\\Data Science Resources\\\\Python\\\\Projects\\\\phrack\")\n",
    "    script_dir = str(os.getcwd())\n",
    "    result_dir = (script_dir + '\\\\result')\n",
    "    data_dir = (script_dir + '\\data')\n",
    "\n",
    "    #modify working directory to the foder where you keep all files relative to the script\n",
    "    modify_working_dir(data_dir)\n",
    "\n",
    "    if create_csv:\n",
    "        print(create_file(get_parsed()))\n",
    "        print(\"csv of parsed prophiles created\")\n",
    "    if create_doc_corpus:\n",
    "        print(create_file(create_corpus()))\n",
    "    if create_topics:\n",
    "        print_top_words(modeling())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder \"C:\\Users\\gaura\\AppData\\Local\\Temp\" will be used to save temporary dictionary and corpus.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-252f1eb6fb13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Folder \"{}\" will be used to save temporary dictionary and corpus.'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTEMP_FOLDER\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"corpus_phrack.p\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "TEMP_FOLDER = tempfile.gettempdir()\n",
    "print('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))\n",
    "\n",
    "from gensim import corpora\n",
    "\n",
    "documents = pickle.load(open(\"corpus_phrack.p\", 'rb'))\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
