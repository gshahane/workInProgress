{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from parse import *\n",
    "from docx import Document\n",
    "from functools import reduce\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "#global variables\n",
    "df_topic_words\n",
    "df_topic_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below modify working directory for accessing data files and creating the result files in separate folders. Filenames function extracts the filenames of the word documents that contain Hackers' interviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modify_working_dir(dir_name):\n",
    "    # Modify the path based on where you keep your docx files\n",
    "    os.chdir(dir_name)\n",
    "\n",
    "def filenames():\n",
    "    # Extracts filenames based on the working directory\n",
    "    #Print working directory for your reference\n",
    "    print(os.getcwd())\n",
    "    files = os.listdir(str(os.getcwd()))\n",
    "    docx = []\n",
    "    for file in files:\n",
    "        if file.endswith(\".docx\"):\n",
    "            docx.append(file)\n",
    "    return docx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The funcion \"word_doc\" extracts text from word documents and combines all the paragraphs into one single continuous line of plain text. This is necessary to create a corpus and further process the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_doc(file):\n",
    "    # Combines paragraphs into one continuous string of text\n",
    "\n",
    "    #print(file)\n",
    "    f = Document(file)\n",
    "    textline = []\n",
    "    for para in f.paragraphs:\n",
    "        textline.append(para.text)\n",
    "    text = reduce(lambda x, y: x + ' ' + y, textline)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions \"parse_text\" and \"get_parsed\" parse the text appeares after the headings hardcoded in a written list at the end of this program. This is required to parse information such as handles, age, date of birth, books and etc mentioned by the hacker in their interviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_text(text, filename):\n",
    "    #Parses the individual document based on the search parameters in global list: list_headings\n",
    "\n",
    "    profile = {}\n",
    "    global list_headings\n",
    "    second = \": {}:\"\n",
    "    for _ in list_headings:\n",
    "        try:\n",
    "            profile[_] = (list(search(_.lower()+second, text.lower()))[0]).rsplit(\"  \",1)[0]\n",
    "        except:\n",
    "            profile[_] = \"None\"\n",
    "            profile[\"filename\"] = filename\n",
    "            pass\n",
    "    return profile\n",
    "\n",
    "def get_parsed():\n",
    "    #Parses all the documents and returns a pandas dataframe\n",
    "\n",
    "    final_dict = []\n",
    "    for filename in filenames():\n",
    "        text = word_doc(str(filename))\n",
    "        p = parse_text(text, filename)\n",
    "        final_dict.append(p)\n",
    "    #print(final_dict)\n",
    "    final_df = pd.DataFrame(final_dict)\n",
    "    df = final_df[list_headings]\n",
    "\n",
    "    return [df,\"dataframe\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we proceed to create corpus with the function \"create_corpus\". \"modelling\" uses the corpus, removes stopwords, runs Scikit-learn's Countvectorizer to count frequencies of the words in individual documents. Later, Scikit Learn's Latent Dirichlet Allocation algorithm (LDA) is used for topic-modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_corpus():\n",
    "    corpus = []\n",
    "    for _ in filenames():\n",
    "        text = word_doc(str(_))\n",
    "        corpus.append(text)\n",
    "    return [corpus, \"corpus\"]\n",
    "\n",
    "def modeling():\n",
    "    #Read Corpus\n",
    "    global topics_for_lda\n",
    "\n",
    "    #Read stopwords\n",
    "    with open(\"stopwords_custom.txt\", 'r') as f:\n",
    "        stopwords = f.read().split()\n",
    "        #print(stopwords)\n",
    "\n",
    "    modify_working_dir(result_dir)\n",
    "    corpus = pickle.load(open(\"corpus_phrack.p\", 'rb'))\n",
    "\n",
    "    #Count Term Frequency\n",
    "    tf_vectorizer = CountVectorizer(stop_words=stopwords, min_df= 5, max_df= 10,  ngram_range=(1,2))\n",
    "    tf = tf_vectorizer.fit_transform(corpus)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    #print(len(tf_feature_names))\n",
    "\n",
    "    #LDA\n",
    "    lda = LatentDirichletAllocation(n_topics=topics_for_lda, max_iter = 500, learning_method='online', random_state=0 )\n",
    "    lda.fit(tf)\n",
    "    doc_word = lda.transform(tf)\n",
    "    #print(lda.transform(tf))\n",
    "\n",
    "    doc_topic_dist_unnormalized = np.matrix(doc_word)\n",
    "\n",
    "    # normalize the distribution\n",
    "    doc_topic_dist = doc_topic_dist_unnormalized / doc_topic_dist_unnormalized.sum(axis=1)\n",
    "\n",
    "    doc_topic_dist = pd.DataFrame(doc_topic_dist*100)\n",
    "    return [lda, tf_feature_names, doc_topic_dist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the processing part of the program is over. We can extract and print the top words for the model as below. The function create Pandas DataFrames to store topics and their distributions across the documents. The DataFrames are written to the \"Result\" folder in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model):\n",
    "    lda = model[0]\n",
    "    feature_names = model[1]\n",
    "    topic_dist = model[2]\n",
    "    \n",
    "    #modify to get more number of top words for each tpic\n",
    "    n_top_words = 20\n",
    "    \n",
    "    #global variables\n",
    "    global df_topic_words, df_topic_dist\n",
    "\n",
    "    list_topic_words = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        topic_words = {}\n",
    "\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        words_for_topic = (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        print(words_for_topic)\n",
    "        topic_words[topic_idx] = words_for_topic\n",
    "        list_topic_words.append(topic_words)\n",
    "\n",
    "    #Read file for filenames and handle names\n",
    "    handles_df = pd.read_csv('handles.csv', sep=',')\n",
    "\n",
    "    # get topic distribution across the documents\n",
    "    list_topic_dist_docs = []\n",
    "\n",
    "    for _ in range(len(topic_dist.index)):\n",
    "        dict_topic_dist = {}\n",
    "        topic_for_doc= list(topic_dist.iloc[_,:])\n",
    "        try:\n",
    "            filename = handles_df.iloc[_,1].strip()\n",
    "            handle = handles_df.iloc[_,2].strip()\n",
    "        except:\n",
    "            handle = _\n",
    "\n",
    "        # Add lines for additional topics, if more than 3,  to be included in the final topic\n",
    "        dict_topic_dist['filename'] = filename\n",
    "        dict_topic_dist['handle'] = handle\n",
    "        for i in range(topics_for_lda):\n",
    "            topic_number = i+1\n",
    "            dict_topic_dist['topic_' + str(topic_number)] = topic_for_doc[i]\n",
    "        dict_topic_dist['top_topic'] = int(topic_for_doc.index(max(topic_for_doc))) + 1\n",
    "\n",
    "        #Append dictionary to the final list for dataframe creation\n",
    "        list_topic_dist_docs.append(dict_topic_dist)\n",
    "\n",
    "    #Sort and generate Topic Distribution File\n",
    "    df_topic_dist = pd.DataFrame(list_topic_dist_docs).sort(columns=\"top_topic\", ascending=True)\n",
    "    modify_working_dir(result_dir)\n",
    "    df_topic_dist.to_csv(\"topic distribution for prophiles.csv\")\n",
    "\n",
    "    #generate topics and their words distribution\n",
    "    df_topic_words = pd.DataFrame(list_topic_words)\n",
    "    df_topic_words.to_csv(\"topic-word distribution.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"create_file\" is a small function that is used for writing corpus and parsed information multiple times in the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_file(list):\n",
    "    # Writes the dataframe to a csv in the working directory\n",
    "    modify_working_dir(result_dir)\n",
    "    data = list[0]\n",
    "    type = list[1]\n",
    "    if type == \"dataframe\":\n",
    "        data.to_csv('prophiles.csv')\n",
    "        data1 = data[['filename', 'Handle']]\n",
    "        data1.to_csv(\"handles.csv\")\n",
    "    elif type == \"corpus\":\n",
    "        pickle.dump(data, open(\"corpus_phrack.p\", 'wb'))\n",
    "    else:\n",
    "        pass\n",
    "    modify_working_dir(data_dir)\n",
    "    return (str(type) + \" created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main part of the program.  \n",
    "\n",
    "**\"list_headings\"** can be modified to add any other sections which need to be parsed from the data.  \n",
    "\n",
    "**\"topics_for_lda = 3\"** can be modified to update the number of desired topics from the Hackers' interview corpus.  \n",
    "\n",
    "The variables mentioned below can be modified (True/False) to enable/disable respective functionalities.  \n",
    "**create_csv = True**         [Creation of parsed CSV file]  \n",
    "**create_doc_corpus = True**  [Creation of document corpus]  \n",
    "**create_topics = True**      [Creation of topic modelling]  \n",
    "\n",
    "The result will be available in \"Result\" folder same as the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    #Add/Remove the headings to be parsed in the order for generating the last parsed dataframe\n",
    "    list_headings = [\"filename\", \"Handle\", \"Past handles\", \"Handle origin\", \"Call him\", \"Call her\", \"Age of your body\",\n",
    "         \"Date of Birth\", \"Produced in\", \"Height\", \"weight\", \"Height & Weight\", \"Eye color\", \"Hair Color\", \"Computers\",\n",
    "         \"Admin of\", \"Sites Frequented\", \"URLs\", \"Women\", \"Cars\", \"Foods\", \"Height & Weight\", \"Projects\", \"Alcohol\",\n",
    "         \"Books & Authors\", \"Authors\", \"Music\", \"Drugs\", \"I like\", \"I dislike\"]\n",
    "\n",
    "    #Variables\n",
    "    topics_for_lda = 3\n",
    "\n",
    "    create_csv = True\n",
    "    create_doc_corpus = True\n",
    "    create_topics = True\n",
    "\n",
    "    #working directories\n",
    "    script_dir = str(os.getcwd())\n",
    "    result_dir = (script_dir + '\\\\result')\n",
    "    data_dir = (script_dir + '\\data')\n",
    "\n",
    "    #modify working directory to the foder where you keep all files relative to the script\n",
    "    modify_working_dir(data_dir)\n",
    "\n",
    "    if create_csv:\n",
    "        print(create_file(get_parsed()))\n",
    "        print(\"csv of parsed prophiles created\")\n",
    "    if create_doc_corpus:\n",
    "        print(create_file(create_corpus()))\n",
    "    if create_topics:\n",
    "        print_top_words(modeling())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing results again here for the ease of exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
